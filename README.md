# End-to-End Data Engineering Project with dbt, Snowflake & Apache Airflow

## 📌 Overview
This project demonstrates a **complete data engineering pipeline** built using:
- **dbt (Data Build Tool)** for SQL-based data transformations and modeling
- **Snowflake** as the cloud data warehouse
- **Apache Airflow** for workflow orchestration and scheduling
- **Python** for automation and scripting

The pipeline covers:
1. **Data Ingestion** – loading raw data into Snowflake  
2. **Transformation** – building staging, intermediate, and mart layers with dbt  
3. **Orchestration** – scheduling and monitoring workflows using Apache Airflow  

---

## ⚙️ Tech Stack
- **dbt Core** – SQL transformations, data models, testing, and documentation  
- **Snowflake** – scalable, cloud-native data warehouse  
- **Apache Airflow** – DAG-based workflow orchestration  
- **Python** – scripting and automation logic  
- **Git/GitHub** – version control and collaboration  

---

## 📂 Project Structure
